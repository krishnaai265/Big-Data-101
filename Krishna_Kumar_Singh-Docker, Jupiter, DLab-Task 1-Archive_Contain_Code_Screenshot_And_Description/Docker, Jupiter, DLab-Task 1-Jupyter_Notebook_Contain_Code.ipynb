{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://DESKTOP-722JGNJ:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1585322481878)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "import org.apache.spark.sql.DataFrame\r\n",
       "import org.apache.spark.sql.functions._\r\n",
       "createSparkSession: ()org.apache.spark.sql.SparkSession\r\n",
       "stopSparkSession: (sparkSession: org.apache.spark.sql.SparkSession)Unit\r\n",
       "getUserFileLocation: ()String\r\n",
       "displayAndFilterData: (fileData: org.apache.spark.sql.DataFrame)Unit\r\n",
       "readCSVFile: (sparkSession: org.apache.spark.sql.SparkSession, file: String)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.{ DataFrame }\n",
    "import org.apache.spark.sql.functions._\n",
    "/**  \n",
    " * Main.java - This file contain the logic of spark session creation, reading CSV file, Spark data analysis code.\n",
    " * @author  Krishna Kumar Singh\n",
    " * @role Junior Software Engineer\n",
    " * @contact krishna_singh1@epam.com\n",
    " */ \n",
    "def createSparkSession(): SparkSession = {\n",
    "    SparkSession.builder.master(\"local\").appName(\"CSVDataAnalysis\").getOrCreate()\n",
    "}\n",
    "\n",
    "def stopSparkSession(sparkSession: SparkSession){\n",
    "    sparkSession.stop()\n",
    "}\n",
    "\n",
    "def getUserFileLocation(): String = {\n",
    "    println(\"Please Enter the file location For eg:C:\\\\Users\\\\Krishna_Singh1\\\\eclipse-workspace\\\\ExcelToAvroConverter\\\\src\\\\main\\\\resources\\\\train.csv\")\n",
    "    readLine()\n",
    "}\n",
    "\n",
    "def displayAndFilterData(fileData: DataFrame) {\n",
    "    val df = fileData.filter(\"srch_adults_cnt>=2\").groupBy(\"hotel_continent\", \"hotel_country\", \"hotel_market\").count()\n",
    "    df.sort(desc(\"count\")).show(3)\n",
    "}\n",
    "\n",
    "def readCSVFile(sparkSession: SparkSession, file: String): DataFrame = {\n",
    "    println(\"Reading files Started\")\n",
    "    sparkSession.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(file) \n",
    "}     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files Started\n",
      "+---------------+-------------+------------+-------+\n",
      "|hotel_continent|hotel_country|hotel_market|  count|\n",
      "+---------------+-------------+------------+-------+\n",
      "|              2|           50|         628|1445790|\n",
      "|              2|           50|         675|1221803|\n",
      "|              2|           50|         682| 743044|\n",
      "+---------------+-------------+------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sparkSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@52d0ccfd\r\n",
       "file: String = D:\\train.csv\r\n",
       "fileData: org.apache.spark.sql.DataFrame = [date_time: string, site_name: string ... 22 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sparkSession = createSparkSession()   \n",
    "val file = \"D:\\\\train.csv\"\n",
    "val fileData = readCSVFile(sparkSession, file)\n",
    "displayAndFilterData(fileData)\n",
    "stopSparkSession(sparkSession)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
